{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purpose</th>\n",
       "      <th>yr_credit</th>\n",
       "      <th>dti</th>\n",
       "      <th>revol_util_dec</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.837</td>\n",
       "      <td>9.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.094</td>\n",
       "      <td>4.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small_business</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.985</td>\n",
       "      <td>10.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>2.100</td>\n",
       "      <td>37.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.539</td>\n",
       "      <td>38.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          purpose  yr_credit    dti  revol_util_dec  total_acc addr_state  \\\n",
       "0     credit_card       26.0  27.65           0.837        9.0         AZ   \n",
       "1             car       12.0   1.00           0.094        4.0         GA   \n",
       "2  small_business       10.0   8.72           0.985       10.0         IL   \n",
       "3           other       15.0  20.00           2.100       37.0         CA   \n",
       "4           other       15.0  17.94           0.539       38.0         OR   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read off data\n",
    "lend = pd.read_csv('data/lending_ml.csv')\n",
    "display(lend.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get dummy variable for puropes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = pd.get_dummies(lend['purpose'], drop_first=True)\n",
    "lend = pd.concat([lend, purpose], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get dummy variable for addr_state\n",
    "\n",
    "Note that from inference statistic part of the exercise. Some states such as IA and MS have so few records that treating those labels as a feature may run into overfit problems later. We will replace state label that has <200 records as SML to have the model recognize them as low applicant states in the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a crosstab to get count by states\n",
    "table_state = pd.crosstab(lend['addr_state'], lend['target'], margins=True)\n",
    "\n",
    "# filter for states that have less than 200 records, put that into list\n",
    "SML_list = table_state[table_state['All'] < 200].index.tolist()\n",
    "SML = pd.DataFrame(SML_list)\n",
    "SML.to_csv('data/SML.csv', index=False) # to be used later for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend['helper_col'] = lend['addr_state'].isin(SML_list) #this serves as helper column only and will be drop later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if helper column is True, the record is from a small state, replace with 'SML', otherwise no change to the state label\n",
    "lend['state'] = np.where(lend.helper_col == 1, 'SML', lend.addr_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy variables\n",
    "state = pd.get_dummies(lend['state'], drop_first = True)\n",
    "lend = pd.concat([lend, state], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get categorical feature column label for DTI grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate function for grouping dti based on the value range\n",
    "def dti(data):\n",
    "    if data['dti'] <= 5: return 5\n",
    "    elif (data['dti'] > 5) & (data['dti'] <=10) : return 10\n",
    "    elif (data['dti'] > 10) & (data['dti'] <=15) : return 15\n",
    "    elif (data['dti'] > 15) & (data['dti'] <=20) : return 20\n",
    "    elif (data['dti'] > 20) & (data['dti'] <=30) : return 30\n",
    "    else: return 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend['dti_gp'] = lend.apply(dti, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_gp = pd.get_dummies(lend['dti_gp'])\n",
    "lend = pd.concat([lend, dti_gp], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. fillna for missing records under revol_util_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend['revol'] = lend['revol_util_dec'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the redunant columns and form the final dataframe for machine learning\n",
    "lend_fin = lend.drop(['addr_state', 'helper_col', 'dti_gp', 'state', 'dti', 'purpose', 'revol_util_dec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset for feature and target to be fed to machine learning model\n",
    "features = lend_fin.drop(['target'], axis=1).values\n",
    "target = lend_fin.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523261169967756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = .3, random_state=5)\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit(X_train, y_train)\n",
    "# Print the accuracy from the testing data.\n",
    "print(accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score without reguluarization: 85.32%\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "score = cv_score(clf, X_train, y_train)\n",
    "print('Accuracy score without reguluarization: {:.2%}'. format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter: {'C': 0.001}\n",
      "best score: 85.32%\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid search to look for the best C\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "gridsearch = GridSearchCV(estimator=clf, param_grid={'C': Cs}, cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print('best parameter: {}'.format(gridsearch.best_params_))\n",
    "print('best score: {:.2%}'.format(gridsearch.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement on accuracy score. We will still set C in the model to avoid data overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=33)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  32 1571]\n",
      " [ 103 9149]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.02      0.04      1603\n",
      "           1       0.85      0.99      0.92      9252\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     10855\n",
      "   macro avg       0.55      0.50      0.48     10855\n",
      "weighted avg       0.76      0.85      0.79     10855\n",
      "\n",
      "0.8457853523721787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=33)\n",
    "CV_model = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_model.fit(X_train, y_train) # model stuck here\n",
    "# CV_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
