{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purpose</th>\n",
       "      <th>yr_credit</th>\n",
       "      <th>dti</th>\n",
       "      <th>revol_util_dec</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.837</td>\n",
       "      <td>9.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.094</td>\n",
       "      <td>4.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small_business</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.985</td>\n",
       "      <td>10.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>2.100</td>\n",
       "      <td>37.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.539</td>\n",
       "      <td>38.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          purpose  yr_credit    dti  revol_util_dec  total_acc addr_state  \\\n",
       "0     credit_card       26.0  27.65           0.837        9.0         AZ   \n",
       "1             car       12.0   1.00           0.094        4.0         GA   \n",
       "2  small_business       10.0   8.72           0.985       10.0         IL   \n",
       "3           other       15.0  20.00           2.100       37.0         CA   \n",
       "4           other       15.0  17.94           0.539       38.0         OR   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read off data\n",
    "lend = pd.read_csv('data/lending_ml.csv')\n",
    "display(lend.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get dummy variable for puropes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = pd.get_dummies(lend['purpose'], drop_first=True)\n",
    "lend = pd.concat([lend, purpose], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get dummy variable for addr_state\n",
    "\n",
    "Note that from inference statistic part of the exercise. Some states such as IA and MS have so few records that treating those labels as a feature may run into overfit problems later. We will replace state label that has <200 records as SML to have the model recognize them as low applicant states in the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a crosstab to get count by states\n",
    "table_state = pd.crosstab(lend['addr_state'], lend['target'], margins=True)\n",
    "\n",
    "# filter for states that have more than 200 records, put that into list\n",
    "LT_list = table_state[table_state['All'] > 200].index.tolist()\n",
    "LT = pd.DataFrame(LT_list) # to be used later for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend['helper_col'] = ~(lend['addr_state'].isin(LT_list)) #this serves as helper column only and will be drop later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if helper column is True, the record is from a small state, replace with 'SML', otherwise no change to the state label\n",
    "lend['state'] = np.where(lend.helper_col == 1, 'SML', lend.addr_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy variables\n",
    "state = pd.get_dummies(lend['state'], drop_first = True)\n",
    "lend = pd.concat([lend, state], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get categorical feature column label for DTI grouping\n",
    "    - In inference statistic portion, DTI at the individual level does not show significance on influencing the paid off rate, but at the binning level it does. Hence, we will create a new column to hold the categorical DTI.\n",
    "    - We will, however, test both with or without binning to see if it actually improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate function for grouping dti based on the value range\n",
    "def dti(data):\n",
    "    if data['dti'] <= 5: return 5\n",
    "    elif (data['dti'] > 5) & (data['dti'] <=10) : return 10\n",
    "    elif (data['dti'] > 10) & (data['dti'] <=15) : return 15\n",
    "    elif (data['dti'] > 15) & (data['dti'] <=20) : return 20\n",
    "    elif (data['dti'] > 20) & (data['dti'] <=30) : return 30\n",
    "    else: return 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend['dti_gp'] = lend.apply(dti, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_gp = pd.get_dummies(lend['dti_gp'])\n",
    "lend = pd.concat([lend, dti_gp], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. fillna for missing records under revol_util_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend['revol'] = lend['revol_util_dec'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the redunant columns and form the final dataframe for machine learning\n",
    "lend_fin = lend.drop(['addr_state', 'helper_col', 'dti_gp', 'dti', 'state', 'purpose', 'revol_util_dec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36182, 52)\n",
      "(36182,)\n"
     ]
    }
   ],
   "source": [
    "# creating dataset for feature and target to be fed to machine learning model\n",
    "features = lend_fin.drop(['target'], axis=1).values\n",
    "target = lend_fin.target.values\n",
    "\n",
    "# check shape of the features and target sets\n",
    "print(features.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for feature engineering steps above to streamline future data processing\n",
    "def data_clean(dataset):\n",
    "    purpose = pd.get_dummies(dataset['purpose'], drop_first=True)\n",
    "    dataset['helper_col'] = ~(dataset['addr_state'].isin(LT_list))\n",
    "    dataset['state'] = np.where(dataset.helper_col == 1, 'SML', dataset.addr_state)\n",
    "    state = pd.get_dummies(dataset['state'], drop_first = True)\n",
    "    dataset['dti_gp'] = dataset.apply(dti, axis=1)\n",
    "    dti_gp = pd.get_dummies(lend['dti_gp'])\n",
    "    dataset['revol'] = dataset['revol_util_dec'].fillna(0)\n",
    "    dataset = pd.concat([dataset, purpose, state, dti_gp], axis=1)    \n",
    "    dataset = lend.drop(['addr_state', 'helper_col', 'dti_gp', 'dti', 'state', 'purpose', 'revol_util_dec'], axis=1)\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Loan Paid off rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85.29%'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:.2%}'.format(np.sum(target)/len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the baseline accuracy, in which a model of guessing 1s for target will achieve 85.29% accuracy. We are dealing with imbalance sample data. The model, therefore, need to have accuracy that is significantly higher to prove that the model is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test set to be used for all models training\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = .3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.85\n",
      "Recall 1.0\n",
      "F1 score 0.92\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit(X_train, y_train)\n",
    "# Print the accuracy from the testing data.\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "print('Precision {:.2}'.format(precision_score(y_test, y_pred_clf)))\n",
    "print('Recall {:.2}'.format(recall_score(y_test, y_pred_clf)))\n",
    "print('F1 score {:.2}'.format(f1_score(y_test, y_pred_clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "def cv_score(clf, x, y, score_func=f1_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score without reguluarization: 92.08%\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "score = cv_score(clf, X_train, y_train)\n",
    "print('F1 score without reguluarization: {:.2%}'. format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With accuracy almost like the baseline, this base logistic regression does not help on improving the prediction of default rate. F1 score is 0.92 and we will use this as baseline benchmark and try improve on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter: {'C': 0.001}\n",
      "best score: 92.08%\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid search to look for the best C\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "gridsearch = GridSearchCV(estimator=clf, param_grid={'C': Cs}, cv=5, scoring='f1')\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print('best parameter: {}'.format(gridsearch.best_params_))\n",
    "print('best score: {:.2%}'.format(gridsearch.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement on accuracy score. We will still set C in the model to avoid data overfit. However, Logistics Regression does not appear to help on predicting loan default rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Random Forest\n",
    "\n",
    "#### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt', random_state=33)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_rf = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  32 1571]\n",
      " [ 103 9149]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.02      0.04      1603\n",
      "           1       0.85      0.99      0.92      9252\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     10855\n",
      "   macro avg       0.55      0.50      0.48     10855\n",
      "weighted avg       0.76      0.85      0.79     10855\n",
      "\n",
      "F1 Score is 91.62%\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred_rf))  \n",
    "print(classification_report(y_test,y_pred_rf))  \n",
    "print('F1 Score is {:.2%}'.format(f1_score(y_test, y_pred_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest baseline model, unfortunately, preform worst than baseline accuracy. We will identify parameters which may help improving the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth' : [4, 6, 8, 10],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=33, bootstrap=True, max_features='sqrt')\n",
    "CV_model = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CV_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 4, 'n_estimators': 100}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=33, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting using the model with the best estimators\n",
    "y_pred_tune = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is 92.03%\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score is {:.2%}'.format(f1_score(y_test,y_pred_tune)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy did not necessary improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              purpose  yr_credit    dti  revol_util_dec  total_acc addr_state  \\\n",
      "0  debt_consolidation         24  15.55           0.444       22.0         MA   \n",
      "1  debt_consolidation         15  16.73           0.545       41.0         NY   \n",
      "2  debt_consolidation         22  15.75           0.346       16.0         CO   \n",
      "3  debt_consolidation         19  18.55           0.546       31.0         CA   \n",
      "4  debt_consolidation         14  27.06           0.709       17.0         CA   \n",
      "\n",
      "   target  \n",
      "0     1.0  \n",
      "1     1.0  \n",
      "2     0.0  \n",
      "3     1.0  \n",
      "4     1.0  \n",
      "(188180, 7)\n"
     ]
    }
   ],
   "source": [
    "# importing error test data\n",
    "test_error = pd.read_csv('data\\lending_test.csv')\n",
    "print(test_error.head())\n",
    "print(test_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_clean = data_clean(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obatain target and features from the error test set\n",
    "features_test = test_error_clean.drop(['target'], axis=1).values\n",
    "target_test = test_error.target.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
